Task 1: Get Language Pair
1. Dataset Selection
We will use the opus100 dataset for translation between English and Vietnamese. This dataset is sourced from the Hugging Face datasets library.

Dataset Source:

Hugging Face datasets library: https://huggingface.co/datasets/Helsinki-NLP/opus-100
2. Dataset Preparation
The process of preparing the dataset involves several steps, including text normalization, tokenization, and word segmentation. Below are the detailed steps and the libraries used for these tasks.

2.1 Text Normalization
Text normalization involves converting text to a standard format. This includes converting text to lowercase, removing punctuation, and handling special characters.

2.2 Tokenization
Tokenization is the process of splitting text into individual tokens (words or subwords). For English, we use the spacy library, and for Vietnamese, we use the pyvi library.

2.3 Word Segmentation
Vietnamese requires special handling for word segmentation. We use the pyvi library for this purpose.

Libraries and Tools:

spacy for English tokenization: spacy.io
pyvi for Vietnamese tokenization: pyvi

Task3:
In this project, we implemented and evaluated three different attention mechanisms (General, Multiplicative, and Additive) in a sequence-to-sequence (Seq2Seq) model for translating between English and Vietnamese. The tasks were divided into three parts:

Implementation of Seq2Seq Model with Attention Mechanisms:

Implemented a Seq2Seq model with three different attention mechanisms.
Loaded and tokenized the dataset.
Built vocabulary and special symbols.
Training and Evaluation:

Defined training and validation functions.
Trained the models with each attention mechanism.
Evaluated the models based on training loss, validation loss, and perplexity.
Evaluation and Verification:

Compared the performance of the attention mechanisms in terms of translation accuracy and computational efficiency.
Provided performance plots showing training and validation loss for each type of attention mechanism.
Displayed attention maps to understand how the model focuses on different parts of the input sequence.
Analyzed the results and discussed the effectiveness of the selected attention mechanism in translating between Vietnamese and English.
Results
Performance Metrics:

General Attention: Training Loss: 0.9585, Validation Loss: 1.0382, Perplexity: 2.7512, Validation Perplexity: 2.7960
Multiplicative Attention: Training Loss: 0.9618, Validation Loss: 1.0387, Perplexity: 2.7542, Validation Perplexity: 2.8137
Additive Attention: Training Loss: 0.9362, Validation Loss: 1.0579, Perplexity: 2.6640, Validation Perplexity: 2.7561
Performance Plots:

Plotted training and validation losses for each attention mechanism to visualize and compare their learning curves.
Attention Maps:

Displayed attention maps to provide insights into the interpretability of the models and understand how they focus on different parts of the input sequence during translation.
Analysis:

The additive attention mechanism showed the best performance in terms of training loss and perplexity but had higher validation loss, suggesting potential overfitting.
The general and multiplicative attention mechanisms had similar performance, with the general attention slightly better in terms of validation perplexity, indicating better generalization.
